{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd51ace-e4ab-40ce-873a-5a21061522d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------#\n",
    "#   进行训练前需要利用这个文件生成cls_train.txt\n",
    "#------------------------------------------------#\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ff7dfa-8289-4aaa-a319-f2aca103d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #---------------------#\n",
    "    #   训练集所在的路径\n",
    "    #---------------------#\n",
    "    datasets_path   = \"/root/autodl-nas/data/facenet/datasets\"\n",
    "\n",
    "    types_name      = os.listdir(datasets_path)\n",
    "    types_name      = sorted(types_name)\n",
    "\n",
    "    list_file = open('cls_train.txt', 'w')\n",
    "    for cls_id, type_name in enumerate(types_name):\n",
    "        photos_path = os.path.join(datasets_path, type_name)\n",
    "        if not os.path.isdir(photos_path):\n",
    "            continue\n",
    "        photos_name = os.listdir(photos_path)\n",
    "\n",
    "        for photo_name in photos_name:\n",
    "            list_file.write(str(cls_id) + \";\" + '%s'%(os.path.join(os.path.abspath(datasets_path), type_name, photo_name)))\n",
    "            list_file.write('\\n')\n",
    "    list_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad52961-1bf8-48e9-a424-80ef4e4c9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss            = triplet_loss()\n",
    "#----------------------#\n",
    "#   记录Loss\n",
    "#----------------------#\n",
    "if local_rank == 0:\n",
    "    loss_history = LossHistory(save_dir, model, input_shape=input_shape)\n",
    "else:\n",
    "    loss_history = None\n",
    "\n",
    "#------------------------------------------------------------------#\n",
    "#   torch 1.2不支持amp，建议使用torch 1.7.1及以上正确使用fp16\n",
    "#   因此torch1.2这里显示\"could not be resolve\"\n",
    "#------------------------------------------------------------------#\n",
    "if fp16:\n",
    "    from torch.cuda.amp import GradScaler as GradScaler\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "model_train     = model.train()\n",
    "#----------------------------#\n",
    "#   多卡同步Bn\n",
    "#----------------------------#\n",
    "if sync_bn and ngpus_per_node > 1 and distributed:\n",
    "    model_train = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model_train)\n",
    "elif sync_bn:\n",
    "    print(\"Sync_bn is not support in one gpu or not distributed.\")\n",
    "\n",
    "if Cuda:\n",
    "    if distributed:\n",
    "        #----------------------------#\n",
    "        #   多卡平行运行\n",
    "        #----------------------------#\n",
    "        model_train = model_train.cuda(local_rank)\n",
    "        model_train = torch.nn.parallel.DistributedDataParallel(model_train, device_ids=[local_rank], find_unused_parameters=True)\n",
    "    else:\n",
    "        model_train = torch.nn.DataParallel(model)\n",
    "        cudnn.benchmark = True\n",
    "        model_train = model_train.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89270580-de85-4b5f-a840-979a4fa6a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------#\n",
    "#   LFW估计\n",
    "#---------------------------------#\n",
    "LFW_loader = torch.utils.data.DataLoader(\n",
    "    LFWDataset(dir=lfw_dir_path, pairs_path=lfw_pairs_path, image_size=input_shape), batch_size=32, shuffle=False) if lfw_eval_flag else None\n",
    "\n",
    "#-------------------------------------------------------#\n",
    "#   0.01用于验证，0.99用于训练\n",
    "#-------------------------------------------------------#\n",
    "val_split = 0.01\n",
    "with open(annotation_path,\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "np.random.seed(10101)\n",
    "np.random.shuffle(lines)\n",
    "np.random.seed(None)\n",
    "num_val = int(len(lines)*val_split)\n",
    "num_train = len(lines) - num_val\n",
    "\n",
    "show_config(\n",
    "    num_classes = num_classes, backbone = backbone, model_path = model_path, input_shape = input_shape, \\\n",
    "    Init_Epoch = Init_Epoch, Epoch = Epoch, batch_size = batch_size, \\\n",
    "    Init_lr = Init_lr, Min_lr = Min_lr, optimizer_type = optimizer_type, momentum = momentum, lr_decay_type = lr_decay_type, \\\n",
    "    save_period = save_period, save_dir = save_dir, num_workers = num_workers, num_train = num_train, num_val = num_val\n",
    ")\n",
    "\n",
    "if True:\n",
    "    if batch_size % 3 != 0:\n",
    "        raise ValueError(\"Batch_size must be the multiple of 3.\")\n",
    "    #-------------------------------------------------------------------#\n",
    "    #   判断当前batch_size，自适应调整学习率\n",
    "    #-------------------------------------------------------------------#\n",
    "    nbs             = 64\n",
    "    lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 1e-1\n",
    "    lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n",
    "    Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "    Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "\n",
    "    #---------------------------------------#\n",
    "    #   根据optimizer_type选择优化器\n",
    "    #---------------------------------------#\n",
    "    optimizer = {\n",
    "        'adam'  : optim.Adam(model.parameters(), Init_lr_fit, betas = (momentum, 0.999), weight_decay = weight_decay),\n",
    "        'sgd'   : optim.SGD(model.parameters(), Init_lr_fit, momentum=momentum, nesterov=True, weight_decay = weight_decay)\n",
    "    }[optimizer_type]\n",
    "\n",
    "    #---------------------------------------#\n",
    "    #   获得学习率下降的公式\n",
    "    #---------------------------------------#\n",
    "    lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, Epoch)\n",
    "\n",
    "    #---------------------------------------#\n",
    "    #   判断每一个世代的长度\n",
    "    #---------------------------------------#\n",
    "    epoch_step      = num_train // batch_size\n",
    "    epoch_step_val  = num_val // batch_size\n",
    "\n",
    "    if epoch_step == 0 or epoch_step_val == 0:\n",
    "        raise ValueError(\"数据集过小，无法继续进行训练，请扩充数据集。\")\n",
    "\n",
    "    #---------------------------------------#\n",
    "    #   构建数据集加载器。\n",
    "    #---------------------------------------#\n",
    "    train_dataset   = FacenetDataset(input_shape, lines[:num_train], num_classes, random = True)\n",
    "    val_dataset     = FacenetDataset(input_shape, lines[num_train:], num_classes, random = False)\n",
    "\n",
    "    if distributed:\n",
    "        train_sampler   = torch.utils.data.distributed.DistributedSampler(train_dataset, shuffle=True,)\n",
    "        val_sampler     = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False,)\n",
    "        batch_size      = batch_size // ngpus_per_node\n",
    "        shuffle         = False\n",
    "    else:\n",
    "        train_sampler   = None\n",
    "        val_sampler     = None\n",
    "        shuffle         = True\n",
    "\n",
    "    gen             = DataLoader(train_dataset, shuffle=shuffle, batch_size=batch_size//3, num_workers=num_workers, pin_memory=True,\n",
    "                            drop_last=True, collate_fn=dataset_collate, sampler=train_sampler)\n",
    "    gen_val         = DataLoader(val_dataset, shuffle=shuffle, batch_size=batch_size//3, num_workers=num_workers, pin_memory=True,\n",
    "                            drop_last=True, collate_fn=dataset_collate, sampler=val_sampler)\n",
    "\n",
    "    for epoch in range(Init_Epoch, Epoch):\n",
    "        if distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "\n",
    "        set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n",
    "\n",
    "        fit_one_epoch(model_train, model, loss_history, loss, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, Epoch, Cuda, LFW_loader, batch_size//3, lfw_eval_flag, fp16, scaler, save_period, save_dir, local_rank)\n",
    "\n",
    "    if local_rank == 0:\n",
    "        loss_history.writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
